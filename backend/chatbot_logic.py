import os
import json
import re
from typing import TypedDict, List, Optional, Dict
import sys
import traceback

# Pydantic ë° LangChain í˜¸í™˜ì„±ì„ ìœ„í•œ ì„í¬íŠ¸
from pydantic import BaseModel, Field, PrivateAttr

# LangChain ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders.base import BaseLoader
from langchain_community.vectorstores import Chroma
from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.documents.compressor import BaseDocumentCompressor
from langchain_core.callbacks.base import Callbacks


# FlashRank ì„í¬íŠ¸
try:
    from flashrank import Ranker, RerankRequest
except ImportError:
    print("FlashRank ë˜ëŠ” ê´€ë ¨ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Reranking ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
    Ranker = None

# --- ì„¤ì • ë° ëª¨ë¸ ì •ì˜ ---
MAX_CORRECTION_ATTEMPTS = 3

# --- ì „ì—­ ë³€ìˆ˜ ë° í—¬í¼ í•¨ìˆ˜ ---
llm_reasoning = None # gpt-5 (ê³ ì„±ëŠ¥ ì¶”ë¡ ìš©)
llm_fast = None      # gpt-4.1 (ë‹¨ìˆœ ì‘ì—…ìš©)
retrievers = {}
approved_templates = []
rejected_templates = []

class CustomRuleLoader(BaseLoader):
    def __init__(self, file_path: str, encoding: str = 'utf-8'):
        self.file_path = file_path
        self.encoding = encoding
    
    def load(self) -> List[Document]:
        docs = []
        try:
            with open(self.file_path, 'r', encoding=self.encoding) as f:
                content = f.read()
        except FileNotFoundError:
            print(f"ğŸš¨ ê²½ê³ : '{self.file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        rule_blocks = re.findall(r'\[ê·œì¹™ ì‹œì‘\](.*?)\[ê·œì¹™ ë\]', content, re.DOTALL)
        for block in rule_blocks:
            lines = block.strip().split('\n')
            metadata = {}
            page_content = ""
            is_content_section = False
            
            for line in lines:
                if line.lower().startswith('content:'):
                    is_content_section = True
                    page_content += line[len('content:'):].strip() + "\n"
                    continue
                if is_content_section:
                    page_content += line.strip() + "\n"
                else:
                    if ':' in line:
                        key, value = line.split(':', 1)
                        metadata[key.strip()] = value.strip()
            
            if page_content:
                docs.append(Document(page_content=page_content.strip(), metadata=metadata))
        
        return docs

class TemplateAnalysisResult(BaseModel):
    status: str = Field(description="í…œí”Œë¦¿ì˜ ìµœì¢… ìƒíƒœ")
    reason: str = Field(description="ìƒì„¸í•œ íŒë‹¨ ì´ìœ ")
    evidence: Optional[str] = Field(None, description="íŒë‹¨ ê·¼ê±° ê·œì¹™ë“¤ì˜ rule_id")
    suggestion: Optional[str] = Field(None, description="ê°œì„  ì œì•ˆ")

class FlashRankRerank(BaseDocumentCompressor):
    _ranker: Ranker = PrivateAttr()
    top_n: int = 3
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if Ranker:
            self._ranker = Ranker()
    
    class Config:
        arbitrary_types_allowed = True
    
    def compress_documents(self, documents: List[Document], query: str, callbacks: Callbacks | None = None) -> List[Document]:
        if not documents or not Ranker:
            return documents[:self.top_n]
        
        rerank_request = RerankRequest(
            query=query,
            passages=[{"id": i, "text": doc.page_content, "metadata": doc.metadata} for i, doc in enumerate(documents)]
        )
        reranked_results = self._ranker.rerank(rerank_request)
        
        final_docs = []
        for item in reranked_results[:self.top_n]:
            doc = documents[item['id']]
            doc.metadata["relevance_score"] = item['score']
            final_docs.append(doc)
        
        return final_docs

class Variable(BaseModel):
    name: str = Field(description="ì¶”ì¶œëœ ë³€ìˆ˜ì˜ í•œê¸€ ì´ë¦„ (ì˜ˆ: ë§¤ì¥ëª…, íì ì¼ì). `#{{}}`ì— ë“¤ì–´ê°ˆ ë¶€ë¶„ì…ë‹ˆë‹¤.")
    original_value: str = Field(description="ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œëœ ì‹¤ì œ ê°’")
    description: str = Field(description="í•´ë‹¹ ë³€ìˆ˜ì— ëŒ€í•œ ê°„ë‹¨í•œ í•œê¸€ ì„¤ëª… (ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡)")

class ParameterizedResult(BaseModel):
    parameterized_template: str = Field(description="íŠ¹ì • ì •ë³´ê°€ #{{ë³€ìˆ˜ëª…}}ìœ¼ë¡œ ëŒ€ì²´ëœ ìµœì¢… í…œí”Œë¦¿")
    variables: List[Variable] = Field(description="ì¶”ì¶œëœ ë³€ìˆ˜ë“¤ì˜ ëª©ë¡")

class StructuredTemplate(BaseModel):
    title: str = Field(description="í…œí”Œë¦¿ì˜ ì œëª© ë˜ëŠ” ì²« ë¬¸ì¥")
    body: str = Field(description="ì œëª©ê³¼ ë²„íŠ¼ í…ìŠ¤íŠ¸ë¥¼ ì œì™¸í•œ í…œí”Œë¦¿ì˜ í•µì‹¬ ë³¸ë¬¸ ë‚´ìš©. ì¤„ë°”ê¿ˆì´ ìˆë‹¤ë©´ \\nìœ¼ë¡œ ìœ ì§€í•´ì£¼ì„¸ìš”.")
    buttons: Optional[List[tuple[str, str]]] = Field(None, description="í…œí”Œë¦¿ì— í¬í•¨ë  ë²„íŠ¼ ë¦¬ìŠ¤íŠ¸. ì˜ˆ: [('ì›¹ì‚¬ì´íŠ¸', 'ìì„¸íˆ ë³´ê¸°')]")


def load_line_by_line(file_path: str) -> List[str]:
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            items = [line.strip() for line in f if line.strip()]
        return items
    except FileNotFoundError:
        return []

def load_by_separator(file_path: str, separator: str = '---') -> List[str]:
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        items = [section.strip() for section in content.split(separator) if section.strip()]
        return items
    except FileNotFoundError:
        return []




# def render_template_from_structured(data: StructuredTemplate) -> StructuredTemplate:
#     return data


def initialize_system():
    global llm_reasoning, llm_fast, llm_medium, retrievers, approved_templates, rejected_templates
    if llm_reasoning is not None:
        return
        
    print("ì„œë²„ ì‹œì‘: ì‹œìŠ¤í…œ ì´ˆê¸°í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤...")
    try:
        data_dir = 'data'
        
        llm_reasoning =ChatOpenAI(model="gpt-4.1", temperature=0.2)
        llm_medium = ChatOpenAI(model="gpt-4.1-nano", temperature=0.2)
        llm_fast = ChatOpenAI(model="gpt-4.1-nano", temperature=0)

        
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        
        approved_templates = load_line_by_line(os.path.join(data_dir, "approved_templates.txt"))
        rejected_templates = load_by_separator(os.path.join(data_dir, "rejected_templates.txt"))
        
        docs_compliance = CustomRuleLoader(os.path.join(data_dir, "compliance_rules.txt")).load()
        docs_generation = CustomRuleLoader(os.path.join(data_dir, "generation_rules.txt")).load()
        docs_whitelist = [Document(page_content=t) for t in approved_templates]
        docs_rejected = [Document(page_content=t) for t in rejected_templates]
        
        def create_db(name, docs):
            if not docs:
                print(f"ğŸš¨ '{name}'ì— ëŒ€í•œ ë¬¸ì„œê°€ ì—†ì–´ DB ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return None
            
            print(f"âœ¨ '{name}' ì»¬ë ‰ì…˜ì„ ì¸ë©”ëª¨ë¦¬ DBì— ìƒì„±í•©ë‹ˆë‹¤...")
            db = Chroma.from_documents(
                docs, 
                embeddings, 
                collection_name=name
            )
            print(f"ğŸ’¾ '{name}' ì»¬ë ‰ì…˜ì´ ì¸ë©”ëª¨ë¦¬ DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return db

        db_compliance = create_db("compliance_rules", docs_compliance)
        db_generation = create_db("generation_rules", docs_generation)
        db_whitelist = create_db("whitelist_templates", docs_whitelist)
        db_rejected = create_db("rejected_templates", docs_rejected)
        
        def create_hybrid_retriever(vectorstore, docs):
            if not vectorstore:
                return None
            vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
            if docs:
                keyword_retriever = BM25Retriever.from_documents(docs)
                keyword_retriever.k = 5
                ensemble_retriever = EnsembleRetriever(
                    retrievers=[vector_retriever, keyword_retriever], weights=[0.5, 0.5]
                )
            else:
                ensemble_retriever = vector_retriever
            if Ranker:
                compressor = FlashRankRerank(top_n=5)
                return ContextualCompressionRetriever(base_compressor=compressor, base_retriever=ensemble_retriever)
            return ensemble_retriever

        retrievers['compliance'] = create_hybrid_retriever(db_compliance, docs_compliance)
        retrievers['generation'] = create_hybrid_retriever(db_generation, docs_generation)
        retrievers['whitelist'] = create_hybrid_retriever(db_whitelist, docs_whitelist)
        retrievers['rejected'] = create_hybrid_retriever(db_rejected, docs_rejected)

        for name, retriever in retrievers.items():
            if retriever:
                print(f"âœ… '{name}' ë¦¬íŠ¸ë¦¬ë²„ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                print(f"ğŸš¨ ê²½ê³ : '{name}' ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (ê´€ë ¨ ë°ì´í„° íŒŒì¼ ë¶€ì¬ ì¶”ì •).")
        print("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ.")

    except Exception as e:
        print(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise e


def process_chat_message(message: str, state: dict) -> dict:
    try:
        if 'step' not in state:
            state['step'] = 'initial'
            state['hasImage'] = False
        if state['step'] == 'initial':
            if 'original_request' not in state:
                state['original_request'] = message
            state['step'] = 'recommend_templates'
            if 'whitelist' not in retrievers or not retrievers['whitelist']:
                state['step'] = 'select_style'
                return {'message': 'ìœ ì‚¬ í…œí”Œë¦¿ ê²€ìƒ‰ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™” ìƒíƒœì…ë‹ˆë‹¤. ìƒˆë¡œìš´ í…œí”Œë¦¿ì„ ìƒì„±í•˜ê² ìŠµë‹ˆë‹¤.\n\nì›í•˜ì‹œëŠ” ìŠ¤íƒ€ì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”:', 'state': state, 'options': ['ê¸°ë³¸í˜•', 'ì´ë¯¸ì§€í˜•', 'ì•„ì´í…œë¦¬ìŠ¤íŠ¸í˜•']}
            similar_docs = retrievers['whitelist'].invoke(state['original_request'])
            if not similar_docs:
                state['step'] = 'select_style'
                return {'message': 'ìœ ì‚¬í•œ ê¸°ì¡´ í…œí”Œë¦¿ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ í…œí”Œë¦¿ì„ ìƒì„±í•˜ê² ìŠµë‹ˆë‹¤.\n\nì›í•˜ì‹œëŠ” ìŠ¤íƒ€ì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”:', 'state': state, 'options': ['ê¸°ë³¸í˜•', 'ì´ë¯¸ì§€í˜•', 'ì•„ì´í…œë¦¬ìŠ¤íŠ¸í˜•']}
            structured_templates = [structure_template_with_llm(doc.page_content) for doc in similar_docs[:3]]
            template_options = [f'í…œí”Œë¦¿ {i+1} ì‚¬ìš©' for i in range(len(similar_docs[:3]))]
            new_creation_options = ['ìƒˆë¡œ ë§Œë“¤ê¸°']
            final_options = template_options + new_creation_options
            state['retrieved_similar_templates'] = [doc.page_content for doc in similar_docs[:3]]
            return {
                'message': 'ìš”ì²­í•˜ì‹  ë‚´ìš©ê³¼ ìœ ì‚¬í•œ ê¸°ì¡´ í…œí”Œë¦¿ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. ì´ í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ì‹œê±°ë‚˜ ìƒˆë¡œ ë§Œë“œì‹œê² ì–´ìš”?', 
                'state': state, 
                'structured_templates': structured_templates, 
                'options': final_options
            }
        elif state['step'] == 'recommend_templates':
            if message.endswith(' ì‚¬ìš©'):
                try:
                    template_idx = int(message.split()[1]) - 1
                    selected_template = state['retrieved_similar_templates'][template_idx]
                    state['selected_template_content'] = selected_template
                    state['step'] = 'generate_and_validate'
                    return process_chat_message(message, state)
                except (IndexError, ValueError):
                    pass
            elif message == 'ìƒˆë¡œ ë§Œë“¤ê¸°':
                state['step'] = 'select_style'
                return {
                    'message': 'ìƒˆë¡œìš´ í…œí”Œë¦¿ì„ ìƒì„±í•©ë‹ˆë‹¤. ì›í•˜ì‹œëŠ” ìŠ¤íƒ€ì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.',
                    'state': state,
                    'options': ['ê¸°ë³¸í˜•', 'ì´ë¯¸ì§€í˜•', 'ì•„ì´í…œë¦¬ìŠ¤íŠ¸í˜•']
                }
            options = [f'í…œí”Œë¦¿ {i+1} ì‚¬ìš©' for i in range(len(state.get('retrieved_similar_templates',[])))] + ['ìƒˆë¡œ ë§Œë“¤ê¸°']
            return {'message': 'ì œì‹œëœ ì˜µì…˜ ì¤‘ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.', 'state': state, 'options': options}
        elif state.get("step") == "select_style":
            if message in ["ê¸°ë³¸í˜•", "ì´ë¯¸ì§€í˜•", "ì•„ì´í…œë¦¬ìŠ¤íŠ¸í˜•"]:
                state["selected_style"] = message
                if message == "ì´ë¯¸ì§€í˜•":
                    state["hasImage"] = True
                    state["step"] = "generate_and_validate"
                    return process_chat_message(message, state)
                elif message == "ê¸°ë³¸í˜•":
                    state["hasImage"] = False
                    state["step"] = "generate_and_validate"
                    return process_chat_message(message, state)
                else:
                    state["step"] = "confirm_image_usage"
                    return {
                        "message": "ì´ë¯¸ì§€ë¥¼ í¬í•¨í•˜ì‹œê² ìŠµë‹ˆê¹Œ?",
                        "state": state,
                        "options": ["ì˜ˆ", "ì•„ë‹ˆì˜¤"]
                    }
            else:
                return {
                    "message": "ì„ íƒí•˜ì‹  ìŠ¤íƒ€ì¼ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 'ê¸°ë³¸í˜•', 'ì´ë¯¸ì§€í˜•', 'ì•„ì´í…œë¦¬ìŠ¤íŠ¸í˜•' ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.",
                    "state": state,
                    "options": ["ê¸°ë³¸í˜•", "ì´ë¯¸ì§€í˜•", "ì•„ì´í…œë¦¬ìŠ¤íŠ¸í˜•"]
                }
        elif state.get("step") == "confirm_image_usage":
            if message in ["ì˜ˆ", "ì•„ë‹ˆì˜¤"]:
                state["hasImage"] = (message == "ì˜ˆ")
                state["step"] = "generate_and_validate"
                return process_chat_message(message, state)
            else:
                return {
                    "message": "ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. 'ì˜ˆ' ë˜ëŠ” 'ì•„ë‹ˆì˜¤'ë¡œë§Œ ë‹µí•´ì£¼ì„¸ìš”.",
                    "state": state,
                    "options": ["ì˜ˆ", "ì•„ë‹ˆì˜¤"]
                }
        elif state.get('step') == 'generate_and_validate':
            if 'selected_template_content' in state:
                base_template = state['selected_template_content']
                state['hasImage'] = '(ì´ë¯¸ì§€ ì˜ì—­:' in base_template
                del state['selected_template_content']
            else:
                generated_result = generate_template(
                    request=state["original_request"],
                    style=state.get("selected_style", "ê¸°ë³¸í˜•")
                )
                base_template = generated_result.parameterized_template
                state["variables_info"] = [v.model_dump() for v in generated_result.variables]
            state["base_template"] = base_template
            template_draft = base_template # fill_template_with_request ì œê±° í›„ í…œí”Œë¦¿ ìì²´ë¥¼ draftë¡œ ì‚¬ìš©
            state["template_draft"] = template_draft
            validation_result = validate_template(template_draft)
            state['validation_result'] = validation_result
            state['correction_attempts'] = 0
            if validation_result['status'] == 'accepted':
                state['step'] = 'completed'
                return process_chat_message(message, state)
            else:
                state['step'] = 'correction'
                return {'message': f'í…œí”Œë¦¿ì„ ìƒì„±í–ˆì§€ë§Œ ê·œì • ìœ„ë°˜ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.\n\në¬¸ì œì : {validation_result["reason"]}\n\nê°œì„  ì œì•ˆ: {validation_result.get("suggestion", "ì—†ìŒ")}\n\nAIê°€ ìë™ìœ¼ë¡œ ìˆ˜ì •í•˜ê² ìŠµë‹ˆë‹¤.', 'state': state}
        elif state['step'] == 'correction':
            if state['correction_attempts'] < MAX_CORRECTION_ATTEMPTS:
                corrected_base_template = correct_template(state)
                state['correction_attempts'] += 1
                validation_result = validate_template(corrected_base_template)
                state["validation_result"] = validation_result
                if validation_result["status"] == "accepted":
                    state['base_template'] = corrected_base_template
                    final_draft = corrected_base_template # fill_template_with_request ì œê±° í›„ í…œí”Œë¦¿ ìì²´ë¥¼ draftë¡œ ì‚¬ìš©
                    state["template_draft"] = final_draft
                    state["step"] = "completed"
                    return process_chat_message(message, state)
                else:
                    state['template_draft'] = corrected_base_template
                    return process_chat_message(message, state)
            else:
                state['step'] = 'manual_correction'
                return {'message': f'AI ìë™ ìˆ˜ì •ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì§ì ‘ ìˆ˜ì •í•˜ì‹œê² ìŠµë‹ˆê¹Œ?', 'state': state, 'options': ['í¬ê¸°í•˜ê¸°']}
        elif state['step'] == 'manual_correction':
            if message == 'í¬ê¸°í•˜ê¸°':
                state['step'] = 'initial'
                return {'message': 'í…œí”Œë¦¿ ìƒì„±ì„ í¬ê¸°í–ˆìŠµë‹ˆë‹¤.', 'state': {'step': 'initial'}}
            else:
                user_corrected_template = message
                validation_result = validate_template(user_corrected_template)
                state['validation_result'] = validation_result
                if validation_result['status'] == 'accepted':
                    state['base_template'] = user_corrected_template
                    final_draft = user_corrected_template # fill_template_with_request ì œê±° í›„ í…œí”Œë¦¿ ìì²´ë¥¼ draftë¡œ ì‚¬ìš©
                    state['template_draft'] = final_draft
                    state['step'] = 'completed'
                    return process_chat_message(message, state)
                else:
                    return {'message': f'ìˆ˜ì •í•˜ì‹  í…œí”Œë¦¿ì—ë„ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ìˆ˜ì •í•´ì£¼ì„¸ìš”.', 'state': state, 'options': ['í¬ê¸°í•˜ê¸°']}
        elif state['step'] == 'completed':
            base_template = state.get("base_template", "")
            variables = state.get("variables_info", [])
            structured_data = structure_template_with_llm(base_template)
            editable_variables = {"parameterized_template": base_template, "variables": variables} if variables else None
            has_image_flag = state.get("hasImage", False)
            response_message = "âœ… í…œí”Œë¦¿ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
            state["step"] = "initial"
            return {
                "message": response_message,
                "state": state,
                "template": base_template,
                "structured_template": structured_data.model_dump(),
                "editable_variables": editable_variables,
                "buttons": structured_data.buttons,
                "hasImage": has_image_flag
            }
        return {'message': 'ì•Œ ìˆ˜ ì—†ëŠ” ìƒíƒœì…ë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.', 'state': {'step': 'initial'}}
    except Exception as e:
        print(f"Error in process_chat_message: {e}")
        traceback.print_exc()
        return {'message': f'ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}', 'state': {'step': 'initial'}}

def structure_template_with_llm(template_string: str) -> StructuredTemplate:
    parser = JsonOutputParser(pydantic_object=StructuredTemplate)

    # --- ìµœì¢… ìˆ˜ì •: 'ì›ë³¸ í…ìŠ¤íŠ¸' ë¶€ë¶„ì˜ ì˜ˆì‹œ ë³€ìˆ˜ê¹Œì§€ ëª¨ë‘ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬ ---
    system_prompt = '''ë‹¹ì‹ ì€ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ í•µì‹¬ êµ¬ì„± ìš”ì†Œë¡œ êµ¬ì¡°í™”í•˜ê³ , ë³¸ë¬¸ì„ ì‚¬ìš©ìê°€ ì½ê¸° ì‰½ê²Œ í¸ì§‘í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

# ì§€ì‹œì‚¬í•­:
1.  í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ì˜ë„ë¥¼ 'title'ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤. ê¸¸ì´ëŠ” ì§§ê³  ê°„ê²°í•˜ê²Œ.
2.  í…ìŠ¤íŠ¸ì— ë²„íŠ¼ ì •ë³´ê°€ ìˆë‹¤ë©´ ë¶„ì„í•˜ì—¬ 'buttons' ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì—†ë‹¤ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ `[]`ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
3.  ë²„íŠ¼ì€ ['ë²„íŠ¼ì¢…ë¥˜', 'ë²„íŠ¼ì´ë¦„'] í˜•ì‹ì˜ íŠœí”Œì´ì–´ì•¼ í•©ë‹ˆë‹¤. ë²„íŠ¼ ì¢…ë¥˜ëŠ” ë‚´ìš©ì— ë§ê²Œ 'ì›¹ì‚¬ì´íŠ¸', 'ì•±ë§í¬', 'ì „í™”í•˜ê¸°' ë“±ìœ¼ë¡œ ì¶”ë¡ í•˜ì„¸ìš”.
4.  ì œëª©ê³¼ ë²„íŠ¼ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë‚´ìš©ì„ 'body'ì˜ ê¸°ë³¸ ì¬ë£Œë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
5.  'body'ì˜ ë‚´ìš©ì„ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ **ì¬êµ¬ì„±í•˜ê³  ê°€ë…ì„±ì„ ë†’ì—¬ì£¼ì„¸ìš”.** ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¥´ì„¸ìš”:
    -   **ë¬¸ë§¥ì˜ íë¦„ì„ íŒŒì•…í•˜ì—¬ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¬¸ë‹¨ì„ ë‚˜ëˆ„ê³ , ì¤„ë°”ê¿ˆ(`\n`)ì„ ì¶”ê°€í•˜ì„¸ìš”.**
    -   ë‚˜ì—´ë˜ëŠ” í•­ëª©(ì˜ˆ: `â–¶`, `â€»`)ì´ ìˆë‹¤ë©´ ê¸€ë¨¸ë¦¬ ê¸°í˜¸('-')ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª©ë¡ìœ¼ë¡œ ë§Œë“œì„¸ìš”.
    -   ì „ì²´ì ìœ¼ë¡œ ë¬¸ì¥ì„ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë‹¤ë“¬ì–´ì£¼ì„¸ìš”.
6.  ìµœì¢… ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì§€ì •ëœ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤. ì„œë¡ ì´ë‚˜ ì¶”ê°€ ì„¤ëª…ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
'''

    human_prompt = '''# ì‹¤ì œ ì‘ì—… ìš”ì²­
-   **ì›ë³¸ í…ìŠ¤íŠ¸:** {raw_text}
-   **ì¶œë ¥ í˜•ì‹ (JSON):** {format_instructions}'''

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", human_prompt)
    ])

    chain = prompt | llm_fast | parser
    try:
        structured_data_dict = chain.invoke({
            "raw_text": template_string,
            "format_instructions": parser.get_format_instructions()
        })
        return StructuredTemplate(**structured_data_dict)
    except Exception as e:
        print(f"Error during structuring template: {e}")
        return StructuredTemplate(
            title=template_string.split('\n')[0].strip(),
            body=template_string,
            buttons=[]
        )


def generate_template(request: str, style: str = "ê¸°ë³¸í˜•") -> ParameterizedResult:
    def _parameterize_template_internal(template_string: str) -> ParameterizedResult:
        parser = JsonOutputParser(pydantic_object=ParameterizedResult)
        prompt = ChatPromptTemplate.from_template(
            '''ë‹¹ì‹ ì€ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í…œí”Œë¦¿ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
            ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ê³ ìœ ëª…ì‚¬, ë‚ ì§œ, ì¥ì†Œ, ìˆ«ì ë“± êµ¬ì²´ì ì´ê³  ë°”ë€” ìˆ˜ ìˆëŠ” ì •ë³´ë“¤ì„ ì‹ë³„í•˜ì—¬, ì˜ë¯¸ ìˆëŠ” í•œê¸€ ë³€ìˆ˜ëª…ìœ¼ë¡œ ëŒ€ì²´í•´ì£¼ì„¸ìš”.
            # ì§€ì‹œì‚¬í•­
            1. í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ì •ë³´(ëˆ„ê°€, ì–¸ì œ, ì–´ë””ì„œ, ë¬´ì—‡ì„, ì–´ë–»ê²Œ ë“±)ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
            2. ì›ë³¸ ê°’ê³¼ ë³€ìˆ˜ëª…, ê·¸ë¦¬ê³  ê° ë³€ìˆ˜ì— ëŒ€í•œ ì„¤ëª…ì„ í¬í•¨í•˜ëŠ” ë³€ìˆ˜ ëª©ë¡ì„ ìƒì„±í•©ë‹ˆë‹¤.
            3. ìµœì¢… ê²°ê³¼ë¥¼ ì§€ì •ëœ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤. ê·¸ ì™¸ì˜ ì„¤ëª…ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
            # ì›ë³¸ í…ìŠ¤íŠ¸:
            {original_text}
            # ì¶œë ¥ í˜•ì‹ (JSON):
            {format_instructions}
            '''
        )
        chain = prompt | llm_fast | parser
        try:
            result = chain.invoke({
                "original_text": template_string,
                "format_instructions": parser.get_format_instructions(),
            })
            if not isinstance(result, dict):
                result = {"parameterized_template": template_string, "variables": []}
            if "parameterized_template" not in result:
                result["parameterized_template"] = template_string
            if "variables" not in result:
                result["variables"] = []
            return ParameterizedResult(**result)
        except Exception as e:
            print(f"Error during internal parameterization: {e}")
            return ParameterizedResult(parameterized_template=template_string, variables=[])

    try:
        RULES = {
            # ... RULES ë”•ì…”ë„ˆë¦¬ëŠ” ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€ ...
            "ê³µí†µ": '''
        - GEN-PREVIEW-001 (ë¯¸ë¦¬ë³´ê¸° ë©”ì‹œì§€ ì œí•œ): ì±„íŒ…ë°© ë¦¬ìŠ¤íŠ¸ì™€ í‘¸ì‹œì— ë…¸ì¶œë˜ëŠ” ë¬¸êµ¬. í•œ/ì˜ êµ¬ë¶„ ì—†ì´ 40ìê¹Œì§€ ì…ë ¥ ê°€ëŠ¥. ë³€ìˆ˜ ì‘ì„± ë¶ˆê°€.
        - GEN-REVIEW-001 (ì‹¬ì‚¬ ê¸°ë³¸ ì›ì¹™): ì•Œë¦¼í†¡ì€ ì •ë³´í†µì‹ ë§ë²•ê³¼ ì¹´ì¹´ì˜¤ ë‚´ë¶€ ê¸°ì¤€ì— ë”°ë¼ ì‹¬ì‚¬ë˜ë©°, ìŠ¹ì¸ëœ í…œí”Œë¦¿ë§Œ ë°œì†¡ ê°€ëŠ¥.
        - GEN-REVIEW-002 (ì£¼ìš” ë°˜ë ¤ ì‚¬ìœ ): ë³€ìˆ˜ ì˜¤ë¥˜, ê³¼ë„í•œ ë³€ìˆ˜(40ê°œ ì´ˆê³¼) ì‚¬ìš©, ë³€ìˆ˜ë¡œë§Œ ì´ë£¨ì–´ì§„ í…œí”Œë¦¿, ë³€ìˆ˜ê°€ í¬í•¨ëœ ë²„íŠ¼ëª…, ë³€ìˆ˜ê°€ í¬í•¨ëœ ë¯¸ë¦¬ë³´ê¸° ë©”ì‹œì§€ ì„¤ì • ì‹œ ë°˜ë ¤ë¨.
        - GEN-INFO-DEF-001 (ì •ë³´ì„± ë©”ì‹œì§€ì˜ ì •ì˜): ê³ ê°ì˜ ìš”ì²­ì— ì˜í•œ 1íšŒì„± ì •ë³´, ê±°ë˜ í™•ì¸, ê³„ì•½ ë³€ê²½ ì•ˆë‚´ ë“±ì´ í¬í•¨ë¨. ë¶€ìˆ˜ì ìœ¼ë¡œ ê´‘ê³ ê°€ í¬í•¨ë˜ë©´ ì „ì²´ê°€ ê´‘ê³ ì„± ì •ë³´ë¡œ ê°„ì£¼ë¨.
        - GEN-SERVICE-STD-001 (ì•Œë¦¼í†¡ ì„œë¹„ìŠ¤ ê¸°ì¤€): ì•Œë¦¼í†¡ì€ ìˆ˜ì‹ ìì—ê²Œ ë°˜ë“œì‹œ ì „ë‹¬ë˜ì–´ì•¼ í•˜ëŠ” 'ì •í˜•í™”ëœ ì •ë³´ì„±' ë©”ì‹œì§€ì— í•œí•¨.
        - GEN-BLACKLIST-001 (ë¸”ë™ë¦¬ìŠ¤íŠ¸ - í¬ì¸íŠ¸/ì¿ í°): ìˆ˜ì‹ ì ë™ì˜ ì—†ëŠ” í¬ì¸íŠ¸ ì ë¦½/ì†Œë©¸ ë©”ì‹œì§€, ìœ íš¨ê¸°ê°„ì´ ë§¤ìš° ì§§ì€ ì¿ í° ë“±ì€ ë°œì†¡ ë¶ˆê°€.
        - GEN-BLACKLIST-002 (ë¸”ë™ë¦¬ìŠ¤íŠ¸ - ì‚¬ìš©ì í–‰ë™ ê¸°ë°˜): ì¥ë°”êµ¬ë‹ˆ ìƒí’ˆ ì•ˆë‚´, í´ë¦­í–ˆë˜ ìƒí’ˆ ì•ˆë‚´, ìƒì¼ ì¶•í•˜ ë©”ì‹œì§€, ì•± ë‹¤ìš´ë¡œë“œ ìœ ë„ ë“±ì€ ë°œì†¡ ë¶ˆê°€.
        - GEN-GUIDE-001 (ì •ë³´ì„±/ê´‘ê³ ì„± íŒë‹¨ ê¸°ì¤€): íŠ¹ê°€/í• ì¸ ìƒí’ˆ ì•ˆë‚´, í”„ë¡œëª¨ì…˜ ë˜ëŠ” ì´ë²¤íŠ¸ê°€ í˜¼ì¬ëœ ê²½ìš°ëŠ” ê´‘ê³ ì„± ë©”ì‹œì§€ë¡œ íŒë‹¨ë¨.
        ''',
            "ê¸°ë³¸í˜•": {
                "ê·œì¹™": '''
        - GEN-TYPE-001 (ê¸°ë³¸í˜• íŠ¹ì§• ë° ì œí•œ): ê³ ê°ì—ê²Œ ë°˜ë“œì‹œ ì „ë‹¬ë˜ì–´ì•¼ í•˜ëŠ” ì •ë³´ì„± ë©”ì‹œì§€. í•œ/ì˜ êµ¬ë¶„ ì—†ì´ 1,000ìê¹Œì§€ ì…ë ¥ ê°€ëŠ¥í•˜ë©°, ê°œì¸í™”ëœ í…ìŠ¤íŠ¸ ì˜ì—­ì€ #{ë³€ìˆ˜}ë¡œ ì‘ì„±.
        - GEN-TYPE-002 (ë¶€ê°€ ì •ë³´í˜• íŠ¹ì§• ë° ì œí•œ): ê³ ì •ì ì¸ ë¶€ê°€ ì •ë³´ë¥¼ ë³¸ë¬¸ í•˜ë‹¨ì— ì•ˆë‚´. ìµœëŒ€ 500ì, ë³€ìˆ˜ ì‚¬ìš© ë¶ˆê°€, URL í¬í•¨ ê°€ëŠ¥. ë³¸ë¬¸ê³¼ í•©ì³ ì´ 1,000ì ì´ˆê³¼ ë¶ˆê°€.
        - GEN-TYPE-003 (ì±„ë„ì¶”ê°€í˜• íŠ¹ì§• ë° ì œí•œ): ë¹„ê´‘ê³ ì„± ë©”ì‹œì§€ í•˜ë‹¨ì— ì±„ë„ ì¶”ê°€ ìœ ë„. ì•ˆë‚´ ë©˜íŠ¸ëŠ” ìµœëŒ€ 80ì, ë³€ìˆ˜/URL í¬í•¨ ë¶ˆê°€.
        ''',
                "ìŠ¤íƒ€ì¼ ê°€ì´ë“œ": '''
        # ìŠ¤íƒ€ì¼ ì„¤ëª…: í…ìŠ¤íŠ¸ ì¤‘ì‹¬ìœ¼ë¡œ ì •ë³´ë¥¼ ì „ë‹¬í•˜ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ í…œí”Œë¦¿ì…ë‹ˆë‹¤. ê°„ê²°í•˜ê³  ì§ê´€ì ì¸ êµ¬ì„±ìœ¼ë¡œ ê³µì§€, ì•ˆë‚´, ìƒíƒœ ë³€ê²½ ë“± ëª…í™•í•œ ë‚´ìš© ì „ë‹¬ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
        # ëŒ€í‘œ ì˜ˆì‹œ 1 (ì„œë¹„ìŠ¤ ì™„ë£Œ ì•ˆë‚´)
        ì•ˆë…•í•˜ì„¸ìš”, #{ìˆ˜ì‹ ìëª…}ë‹˜. ìš”ì²­í•˜ì‹  #{ì„œë¹„ìŠ¤} ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ì•„ë˜ ë²„íŠ¼ì„ í†µí•´ í™•ì¸í•´ì£¼ì„¸ìš”.
        # ëŒ€í‘œ ì˜ˆì‹œ 2 (ì˜ˆì•½ ë¦¬ë§ˆì¸ë“œ)
        ì•ˆë…•í•˜ì„¸ìš”, #{ìˆ˜ì‹ ìëª…}ë‹˜. ë‚´ì¼(#{ì˜ˆì•½ì¼ì‹œ})ì— ì˜ˆì•½í•˜ì‹  ì„œë¹„ìŠ¤ê°€ ì˜ˆì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ìŠì§€ ë§ê³  ë°©ë¬¸í•´ì£¼ì„¸ìš”.
        '''
            },
            "ì´ë¯¸ì§€í˜•": {
                "ê·œì¹™": '''
        - GEN-STYLE-001 (ì´ë¯¸ì§€í˜• íŠ¹ì§• ë° ì œí•œ): í¬ë§·í™”ëœ ì •ë³´ì„± ë©”ì‹œì§€ë¥¼ ì‹œê°ì ìœ¼ë¡œ ì•ˆë‚´. ê´‘ê³ ì„± ë‚´ìš© í¬í•¨ ë¶ˆê°€. í…œí”Œë¦¿ ë‹¹ í•˜ë‚˜ì˜ ê³ ì •ëœ ì´ë¯¸ì§€ë§Œ ì‚¬ìš© ê°€ëŠ¥.
        - GEN-STYLE-002 (ì´ë¯¸ì§€í˜• ì œì‘ ê°€ì´ë“œ - ì‚¬ì´ì¦ˆ): ê¶Œì¥ ì‚¬ì´ì¦ˆëŠ” 800x400px (JPG, PNG), ìµœëŒ€ 500KB.
        - GEN-STYLE-009 (ì´ë¯¸ì§€ ì €ì‘ê¶Œ ë° ë‚´ìš© ì œí•œ): íƒ€ì¸ì˜ ì§€ì ì¬ì‚°ê¶Œ, ì´ˆìƒê¶Œì„ ì¹¨í•´í•˜ëŠ” ì´ë¯¸ì§€, ë³¸ë¬¸ê³¼ ê´€ë ¨ ì—†ëŠ” ì´ë¯¸ì§€, ê´‘ê³ ì„± ì´ë¯¸ì§€ëŠ” ì ˆëŒ€ ì‚¬ìš© ë¶ˆê°€.
        ''',
                "ìŠ¤íƒ€ì¼ ê°€ì´ë“œ": '''
        # ìŠ¤íƒ€ì¼ ì„¤ëª…: ì‹œê°ì  ìš”ì†Œë¥¼ í™œìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì‹œì„ ì„ ëŒê³  ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì „ë‹¬í•˜ëŠ” í…œí”Œë¦¿ì…ë‹ˆë‹¤. ìƒí’ˆ í™ë³´, ì´ë²¤íŠ¸ ì•ˆë‚´ ë“± ì‹œê°ì  ì„íŒ©íŠ¸ê°€ ì¤‘ìš”í•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.
        # ëŒ€í‘œ ì˜ˆì‹œ 1 (ì‹ ìƒí’ˆ ì¶œì‹œ)
        (ì´ë¯¸ì§€ ì˜ì—­: ìƒˆë¡œ ì¶œì‹œëœ í™”ì¥í’ˆ ë¼ì¸ì—…)
        '''
            }
        }
        compliance_rules = retrievers.get('compliance').invoke(request)
        formatted_rules = "\n".join([f"- {doc.metadata.get('rule_id', 'Unknown')}: {doc.page_content}" for doc in compliance_rules])
        
        prompt = ChatPromptTemplate.from_template(
            '''You are a highly precise, rule-based Kakao Alimtalk Template Generation Bot. Your sole mission is to generate a perfect template draft that strictly adheres to all user requests, style guides, and provided rules.

### Final Goal:
Create a ready-to-use Alimtalk template draft that reflects the user's request, utilizes the features of the selected style, and **complies with every single provided rule without exception.**

### Input Information:
- **User's Original Request:** "{request}"
- **Style to Apply:** {style}
- **Style Guide:** {style_guide}
- **Absolute Rules to Follow:** {rules}

### Execution Steps:
1.  **Analyze Request:** Meticulously analyze the user's original request to identify the core purpose and required information for the template.
2.  **Apply Style:** Refer to the style guide's description and examples to determine the overall structure and tone & manner.
3.  **Ensure Compliance:** Review **every rule** in the `Absolute Rules to Follow` list. Ensure the generated template does not violate any of them. Pay special attention to variable usage rules (e.g., variable names in Korean, no variables in button names).
4.  **Parameterize:** Identify specific, changeable information (e.g., customer names, dates, product names, amounts) and convert it into the `#{{variable_name}}` format. The variable name must be a concise and clear Korean word representing the information (e.g., `#{{ê³ ê°ëª…}}`, `#{{ì£¼ë¬¸ë²ˆí˜¸}}`).
5.  **Output Format:** Your **only** output must be the raw text of the generated template. Do not include any introductory phrases, explanations, markdown code blocks (```), or any text other than the template itself.


### Generated Template Draft:
'''
        )

        chain = prompt | llm_reasoning| StrOutputParser()
        generated_template_text = chain.invoke({
            "request": request,
            "style": style,
            "style_guide": RULES.get(style, {}).get("ìŠ¤íƒ€ì¼ ê°€ì´ë“œ", ""),
            "rules": f'{RULES["ê³µí†µ"]}\n{RULES.get(style, {}).get("ê·œì¹™", "")}\nê´€ë ¨ ê·œì¹™:\n{formatted_rules}'
        })
        return _parameterize_template_internal(generated_template_text.strip())
    except Exception as e:
        print(f"Error in generate_template: {e}")
        return ParameterizedResult(parameterized_template=f"í…œí”Œë¦¿ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {request}", variables=[])


def validate_template(template: str) -> Dict:
    parser = JsonOutputParser(pydantic_object=TemplateAnalysisResult)
    relevant_rules = retrievers['compliance'].invoke(template)
    generation_rules = retrievers['generation'].invoke(template)
    formatted_rules = "\n".join([f"- {doc.metadata.get('source', 'content')}: {doc.page_content}" for doc in relevant_rules])
    prompt = ChatPromptTemplate.from_template(
        '''ë‹¹ì‹ ì€ ì¹´ì¹´ì˜¤ ì•Œë¦¼í†¡ ì‹¬ì‚¬ ê°€ì´ë“œë¼ì¸ì„ ì™„ë²½í•˜ê²Œ ìˆ™ì§€í•œ AI ì‹¬ì‚¬ê´€ì…ë‹ˆë‹¤.
        ì£¼ì–´ì§„ í…œí”Œë¦¿ì´ ëª¨ë“  ê·œì¹™ì„ ì¤€ìˆ˜í•˜ëŠ”ì§€ ê²€ì‚¬í•˜ê³ , ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
        # ê²€ì‚¬í•  í…œí”Œë¦¿:
        ```{template}```
        # ì£¼ìš” ì‹¬ì‚¬ ê·œì¹™:
        {relevant_rules}
        {generation_rules}
        # ì§€ì‹œì‚¬í•­:
        1. í…œí”Œë¦¿ì´ ëª¨ë“  ê·œì¹™ì„ ì¤€ìˆ˜í•˜ë©´ `status`ë¥¼ "accepted"ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
        2. ê·œì¹™ ìœ„ë°˜ ì‚¬í•­ì´ í•˜ë‚˜ë¼ë„ ë°œê²¬ë˜ë©´ `status`ë¥¼ "rejected"ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
        3. "rejected"ì¸ ê²½ìš°, `reason`ì— ì–´ë–¤ ê·œì¹™ì„ ìœ„ë°˜í–ˆëŠ”ì§€ ëª…í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.
        4. `evidence` í•„ë“œì—ëŠ” ìœ„ë°˜ì˜ ê·¼ê±°ê°€ ëœ ê·œì¹™ì˜ `content`ë¥¼ ì •í™•íˆ ê¸°ì¬í•©ë‹ˆë‹¤.
        5. ìœ„ë°˜ ì‚¬í•­ì„ í•´ê²°í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ `suggestion`ì„ ì œê³µí•©ë‹ˆë‹¤.
        6. ìµœì¢… ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì§€ì •ëœ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.
        # ì‹¬ì‚¬ ê²°ê³¼ (JSON):
        {format_instructions}
        '''
    )
    chain = prompt | llm_reasoning | parser
    try:
        result = chain.invoke({
            "template": template,
            "relevant_rules": formatted_rules,
            "generation_rules": generation_rules,
            "format_instructions": parser.get_format_instructions()
        })
        return result
    except Exception as e:
        print(f"Error during validation: {e}")
        return {"status": "error", "reason": "ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"}


# def correct_template(state: dict) -> str:
#     validation_result = state['validation_result']
#     original_template = state['template_draft']
#     prompt = ChatPromptTemplate.from_template(
#         '''ë‹¹ì‹ ì€ í…œí”Œë¦¿ì˜ ë¬¸ì œì ì„ ë¶„ì„í•˜ê³  ìˆ˜ì •í•˜ëŠ” AI ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
#         ì£¼ì–´ì§„ ì›ë³¸ í…œí”Œë¦¿ê³¼ ë°˜ë ¤ ì‚¬ìœ ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ëª¨ë“  ë¬¸ì œë¥¼ í•´ê²°í•œ ìƒˆë¡œìš´ í…œí”Œë¦¿ì„ ì œì•ˆí•˜ì„¸ìš”.
#         # ì›ë³¸ í…œí”Œë¦¿:
#         ```{original_template}```
#         # ë°˜ë ¤ ì‚¬ìœ  ë° ìˆ˜ì • ì œì•ˆ:
#         - ì´ìœ : {reason}
#         - ê·¼ê±°: {evidence}
#         - ì œì•ˆ: {suggestion}
#         # ì§€ì‹œì‚¬í•­:
#         1. ë°˜ë ¤ ì‚¬ìœ ë¥¼ ëª…í™•íˆ ì´í•´í•˜ê³ , ì–´ë–¤ ë¶€ë¶„ì„ ìˆ˜ì •í•´ì•¼ í• ì§€ íŒŒì•…í•©ë‹ˆë‹¤.
#         2. ìˆ˜ì • ì œì•ˆì„ ì°¸ê³ í•˜ì—¬ í…œí”Œë¦¿ì„ ê°œì„ í•©ë‹ˆë‹¤.
#         3. ì›ë³¸ í…œí”Œë¦¿ì˜ ì˜ë„ëŠ” ìµœëŒ€í•œ ìœ ì§€í•˜ë©´ì„œ ë¬¸ì œì ë§Œ í•´ê²°í•´ì•¼ í•©ë‹ˆë‹¤.
#         4. ìµœì¢… ê²°ê³¼ë¬¼ì€ ìˆ˜ì •ëœ í…œí”Œë¦¿ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì–´ë–¤ ì„¤ëª…ë„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
#         # ìˆ˜ì •ëœ í…œí”Œë¦¿:
#         '''
#     )
#     chain = prompt | llm_reasoning | StrOutputParser()
#     try:
#         corrected_template = chain.invoke({
#             "original_template": original_template,
#             "reason": validation_result.get("reason", ""),
#             "evidence": validation_result.get("evidence", ""),
#             "suggestion": validation_result.get("suggestion", "")
#         })
#         return corrected_template.strip()
#     except Exception as e:
#         print(f"Error during correction: {e}")
#         return original_template

def correct_template(state: dict) -> str:
    try:
        attempts = state.get('correction_attempts', 0)
        
        # ì‹œë„ íšŸìˆ˜ì— ë”°ë¼ ë‹¤ë¥¸ ì§€ì‹œì‚¬í•­ì„ ë™ì ìœ¼ë¡œ ì„¤ì •
        if attempts == 0:
            instruction_step = """
            - 1ì°¨ ìˆ˜ì •: 'ë°˜ë ¤ ì‚¬ìœ 'ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê´‘ê³ ì„± í‘œí˜„ì„ ì •ë³´ì„± í‘œí˜„ìœ¼ë¡œ ìˆœí™”í•˜ì„¸ìš”.
            """
        elif attempts == 1:
            instruction_step = """
            - 2ì°¨ ìˆ˜ì •: ë©”ì‹œì§€ì˜ ì£¼ì²´ë¥¼ 'ì‚¬ì—…ì'ì—ì„œ 'ê³ ê°'ìœ¼ë¡œ ì™„ì „íˆ ì „í™˜í•˜ì—¬, ê³ ê°ì´ ì´ ë©”ì‹œì§€ë¥¼ í†µí•´ ì–´ë–¤ ìœ ìš©í•œ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ”ì§€ì— ì´ˆì ì„ ë§ì¶° ìˆ˜ì •í•˜ì„¸ìš”.
            - ìˆ˜ì • ì›ì¹™: "ì •ë³´ë¥¼ ì „ë‹¬ë°›ëŠ” ê³ ê°"ì˜ ì…ì¥ì—ì„œ ìœ ìš©í•˜ê³  ê¼­ í•„ìš”í•œ ë‚´ìš©ë§Œ ë‚¨ê¸°ì„¸ìš”.
            """
        else: # attempts >= 2
            instruction_step = """
            - ìµœì¢… ìˆ˜ì •: 'ì¿ í°', 'í• ì¸', 'ì´ë²¤íŠ¸', 'íŠ¹ê°€' ë“± ì§ì ‘ì ì¸ ê´‘ê³  ìš©ì–´ ì‚¬ìš©ì„ ê¸ˆì§€í•©ë‹ˆë‹¤.
            - ìˆ˜ì • ì›ì¹™: ê³ ê°ì—ê²Œ í•„ìš”í•œ ì •ë³´ ì œê³µ ê´€ì ìœ¼ë¡œ í‘œí˜„ì„ ì „í™˜í•˜ì„¸ìš”. (ì˜ˆ: 'í• ì¸ ì¿ í° ì œê³µ' -> 'íšŒì› í˜œíƒ ì•ˆë‚´')
            """

        correction_prompt_template = """
        ë‹¹ì‹ ì€ ì¹´ì¹´ì˜¤ ì•Œë¦¼í†¡ ì‹¬ì‚¬íŒ€ì˜ ìˆ˜ì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë°˜ë ¤ëœ í…œí”Œë¦¿ì„ ìˆ˜ì •í•˜ëŠ” ì„ë¬´ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        ### [ì„ë¬´ ëª©í‘œ]
        ì£¼ì–´ì§„ 'ë°˜ë ¤ ì‚¬ìœ 'ë¥¼ ì™„ì „íˆ í•´ê²°í•˜ê³ , 'ìµœì´ˆ ìš”ì²­ ì˜ë„'ë¥¼ ë°˜ì˜í•œ ì •ë³´ì„± í…œí”Œë¦¿ì„ ì™„ì„±í•˜ì„¸ìš”.
        
        ### [ë¶„ì„ ì •ë³´]
        -   **ìµœì´ˆ ìš”ì²­ ì˜ë„:** {original_request}
        -   **ë°˜ë ¤ëœ í…œí”Œë¦¿ ì´ˆì•ˆ:**
            ```{rejected_draft}```
        -   **ë°˜ë ¤ ì‚¬ìœ :** {rejection_reason}
        
        ### [ìˆ˜ì • ì§€ì‹œ]
        1.  **ë°˜ë ¤ ì‚¬ìœ  í•´ê²°**: ë°˜ë ¤ëœ ì´ìœ ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ì—¬ í•´ë‹¹ ë¬¸ì œì ì„ ì™„ì „íˆ ì œê±°í•˜ì„¸ìš”.
        2.  **ê´‘ê³ ì„± í‘œí˜„ ì œê±°**: 'í• ì¸', 'íŠ¹ê°€', 'ì´ë²¤íŠ¸', 'ì¿ í°', 'í˜œíƒ'ê³¼ ê°™ì€ ê´‘ê³ ì„± ìš©ì–´ì™€ 'ì§€ê¸ˆ ë°”ë¡œ í™•ì¸í•˜ì„¸ìš”!' ê°™ì€ êµ¬ë§¤ ìœ ë„ ë¬¸êµ¬ë¥¼ ì‚­ì œí•˜ì„¸ìš”.
        3.  **ì •ë³´ì„± ê°•í™”**: ê°ê´€ì ì´ê³  ì‚¬ì‹¤ì ì¸ ì •ë³´(ì£¼ë¬¸ ìƒíƒœ, ì˜ˆì•½ í˜„í™©, ì„œë¹„ìŠ¤ ì•ˆë‚´ ë“±)ë§Œ ë‚¨ê²¨ì„œ ë©”ì‹œì§€ì˜ ì‹ ë¢°ë„ë¥¼ ë†’ì´ì„¸ìš”.
        4.  **ê´€ì  ì „í™˜**: ë©”ì‹œì§€ ì£¼ì²´ë¥¼ 'ì‚¬ì—…ì'ê°€ ì•„ë‹Œ, **'ì •ë³´ë¥¼ ë°›ëŠ” ê³ ê°'**ì˜ ê´€ì ì—ì„œ ìˆ˜ì •í•˜ì„¸ìš”.
        5.  **ê°€ë…ì„± ê°œì„ **: ê°„ê²°í•˜ê³  ëª…í™•í•œ ë¬¸ì²´ë¡œ ë‹¤ë“¬ê³ , í•„ìš”í•œ ê²½ìš° ì¤„ë°”ê¿ˆì„ ì¶”ê°€í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš”.

        {instruction_step}

        ### [ìˆ˜ì •ëœ í…œí”Œë¦¿]
        ì•„ë˜ì— ìµœì¢… ìˆ˜ì •ëœ í…œí”Œë¦¿ ë‚´ìš©ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì–´ë–¤ ì„œë¡ ì´ë‚˜ ì„¤ëª…ë„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
        """
        
        correction_prompt = ChatPromptTemplate.from_template(correction_prompt_template)
        
        # ë™ì  ì§€ì‹œì‚¬í•­ì„ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
        correction_prompt = correction_prompt.partial(instruction_step=instruction_step)
        
        correction_chain = correction_prompt | llm_reasoning | StrOutputParser()

        rejection_info = state['validation_result']['reason']
        if state['validation_result'].get('suggestion'):
            rejection_info += "\nê°œì„  ì œì•ˆ: " + state['validation_result']['suggestion']

        new_draft = correction_chain.invoke({
            "original_request": state['original_request'],
            "rejected_draft": state['template_draft'],
            "rejection_reason": rejection_info
        })
        
        # ì½”ë“œ ë¸”ë¡ ë§ˆí¬ë‹¤ìš´ì„ ì œê±°í•˜ê³  ë‚´ìš©ë§Œ ë°˜í™˜
        return new_draft.strip().strip('"`')

    except Exception as e:
        print(f"Error in correct_template: {e}")
        traceback.print_exc()
        return state.get('template_draft', 'ìˆ˜ì • ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.')